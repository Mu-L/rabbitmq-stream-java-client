= RabbitMQ Stream Java Client
:revnumber: {project-version}
:example-caption!:
ifndef::imagesdir[:imagesdir: images]
ifndef::sourcedir[:sourcedir: ../../main/java]
:source-highlighter: prettify
:test-examples: ../../test/java/com/rabbitmq/stream/docs

The RabbitMQ Stream Java Client is a Java library to communicate with
the https://github.com/rabbitmq/rabbitmq-stream[RabbitMQ Stream Plugin].
It allows to create and delete streams, as well as to publish to and consume from
these streams.

== What is a RabbitMQ Stream?

A RabbitMQ stream is an append-only, FIFO structure. It differs from the classical
RabbitMQ queue in the way message consumption works. In a classical RabbitMQ queue,
consuming removes messages from the queue. In a RabbitMQ stream, consuming leaves
the stream intact. So the content of a stream can read and re-read without
impact or destructive effect.

None of the stream or classical queue data structure is better than the other,
they are usually suited for different use cases.

== When to Use RabbitMQ Stream?

A stream abstraction is useful when one or several consumer applications
require the whole history of data ("replay").

Streams can also be useful when a higher throughput than with classical
RabbitMQ queues is required. RabbitMQ streams use various optimization techniques
and a custom network protocol to achieve performances that are not possible
with the other protocols supported in RabbitMQ (AMQP, STOMP, MQTT). This
does not make the stream protocol better than these protocols, they just
all serve different purposes.

== Other Way to Use Streams in RabbitMQ

It is also possible to use the stream abstraction in RabbitMQ
with the AMQP 0-9-1 protocol. Instead of consuming from a stream
with the stream protocol, one consumes from a _stream queue_ with
the AMQP 0-9-1. A _stream queue_ is a special type of queue that
has been adapted to provide the stream semantics (mainly non-destructive
reading).

Using a stream queue has the advantage to provide the features
inherent to the stream abstraction (append-only structure, non-destructive
reading) with any AMQP 0-9-1 client library. This is clearly
interesting when considering the maturity of AMQP 0-9-1 client libraries
and the ecosystem around AMQP 0-9-1.

But by using a stream queue, one does not benefit from the performance
of the stream protocol, which has been designed for performance in mind,
whereas AMQP 0-9-1 is a more general-purpose protocol.

It is not possible to use stream queues with the stream Java client,
you need to use an AMQP 0-9-1 client library.

== Guarantees

RabbitMQ stream provides at-least-once guarantees thanks to the
publisher confirm mechanism, which is supported by the stream Java client.

== Using the Stream Java Client

The library requires Java 8 or more.

A RabbitMQ node with the stream plugin enabled is required:

* Download the https://bintray.com/rabbitmq/all-dev/rabbitmq-stream[latest generic Unix alpha from Bintray].
* Follow the https://www.rabbitmq.com/install-generic-unix.html[instructions to install the generic Unix package].
* Enable the plugin `./rabbitmq-plugins enable rabbitmq_stream`.
* Start the broker `./rabbitmq-server -detached`. This starts the stream listener on port 5555.

=== Dependencies

Use your favorite build management tool to add the client dependencies to your project.

Note the client uses the https://github.com/apache/qpid-proton-j[Apache QPid Proton-J]
library for AMQP 1.0 message encoding and decoding.

==== Maven

.pom.xml
[source,xml,subs="attributes,specialcharacters"]
----
<dependencies>

  <dependency>
    <groupId>com.rabbitmq</groupId>
    <artifactId>stream-client</artifactId>
    <version>{project-version}</version>
  </dependency>

  <dependency>
    <groupId>org.apache.qpid</groupId>
    <artifactId>proton-j</artifactId>
    <version>{protonj-version}</version>
  </dependency>

</dependencies>

<repositories>

  <repository>
    <id>ossrh</id>
    <url>https://oss.sonatype.org/content/repositories/snapshots</url>
    <snapshots><enabled>true</enabled></snapshots>
    <releases><enabled>false</enabled></releases>
  </repository>

</repositories>
----

==== Gradle

.build.gradle
[source,groovy,subs="attributes,specialcharacters"]
----
dependencies {
  compile "com.rabbitmq:stream-client:{project-version}"
  compile "org.apache.qpid:proton-j:{protonj-version}"
}

repositories {
  maven { url 'https://oss.sonatype.org/content/repositories/snapshots' }
  mavenCentral()
}
----

=== Connecting to RabbitMQ Stream

The main API of the stream Java client is the `com.rabbitmq.stream.Client`
class. To connect to a local instance of the RabbitMQ Stream plugin
with the default user and password, just create an `Client` instance

.Connecting with all the defaults
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=client-creation]
--------
<1> Connect to localhost:5555 with guest / guest

A `Client` constructor can take a `ClientParameters` to set
parameters like host, port, username, and password:

.Connecting with parameters
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=client-creation-with-client-parameters]
--------
<1> Use `ClientParameters` to set connection parameters

=== Managing Streams

Streams can be created with the `Client#create(String)` method:

.Creating a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-creation]
--------
<1> Create the `my-stream` stream
<2> Check if creation is OK
<3> Check response code

The `Constants` class can help to make sense of response codes.

The deletion of a stream follows the same pattern with the
`Client#delete(String)` method:

.Deleting a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-deletion]
--------
<1> Delete the `my-stream` stream
<2> Check if deletion is OK
<3> Check response code

Note you should avoid stream churn (creating and deleting streams
repetitively) as their creation and deletion imply
some significant housekeeping on the server side (interactions
with the file system, communication between nodes of the cluster).
Therefore you should consider streams as long-lived objects.

=== Publishing Messages

The simplest way to publish something to a stream is the
`Client#publish(Stream, byte[])` method:

.Publishing a message
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-simple]
--------
<1> The payload of a message is an array of bytes
<2> The stream to publish to
<3> The content to publish

The `publish` method returns a long: it is the publishing identifier
for the message. This identifier is used to correlate the message
with a publishing confirmation that the broker sends asynchronously.
This allows making sure a message made it to the broker and provides
at-least-once guarantees. Each `Client` instance can register a callback to react
to publish confirms when the instance is created:

.Handling publish confirms
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-confirm-callback]
--------
<1> Set publish confirm callback on client creation

The broker also sends notification for messages that it could not
take into account, for example when the targeted stream does not
exist or is not available. Just like with publish confirms, set
up the listener when creating the client instance:

.Handling publishing errors
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-error-callback]
--------
<1> Set publish error callback on client creation

The `PublishErrorListener` callback provides the publishing ID as well
as an error code to figure out the reason of the publishing error.

NOTE: The publishing identifier scopes the client instance connection, it
is not a global identifier for the message.

It is also possible to publish messages in batch, with the
`publishBinary(List<byte[]>)` method. The method returns a
list of publishing identifiers
to handle publish confirms. The following snippet shows how to use
publish messages in batch:

.Publishing messages in batch
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-multiple]
--------
<1> Messages are a list of byte arrays
<2> The stream to publish to
<3> The messages to publish

TIP: Use batch publishing to improve throughput.

You may wonder if messages are only made of a byte array payload. They are not.
They can have fixed and arbitrary headers. We will see how to
create complex messages in a dedicated section.

=== Consuming Messages

The `Client#subscribe` method allows subscribing to a stream:

.Subscribing to a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=subscribe]
--------
<1> The identifier of the subscription
<2> The stream to consume from
<3> The offset to start consuming from
<4> The initial number of credits
<5> Check if response is OK
<6> Check response code

Once the subscription is done, the broker will start sending messages.
We will see how to handle these messages asynchronously in a callback in a moment.
Let's have a further look at the parameters of `subscribe`:

* Identifier of the subscription: this is an arbitrary number to correlate
the subscription with the inbound messages. The value must be unique within
the connection.
* Stream: this is the stream to consume from. It obviously must exist.
* Offset: this is the offset to start consuming from. 0 means consuming
from the beginning of the stream (first messages). -1 means consuming
from the end of the stream at the time of subscription (last messages). The client
can also specify a number in between, for example the offset where it left off
in a previous incarnation of the application.
* Initial number of credits: the number of chunks of messages the broker can
send immediately after the subscription. This is a way to throttle delivery
in case the broker send messages faster than the client can process them. We will
see more about credits later.

The broker delivers messages in chunks (batches). The size of a chunk is not
fixed, it can vary from a few messages to thousands of them in the same chunk.
Delivering messages in chunks instead of one by one contributes to the higher throughput
RabbitMQ Stream can achieve compared to other protocols like AMQP 0-9-1
(which do use batch at all).

The `Client` provides 2 callbacks to handle inbound messages: one for each
chunk and another one for each message within a chunk. Here is more information
about each callback:

* A `ChunkListener`: this callback is merely technical, it gives
the application the occasion to ask for more chunks of messages
or to compute metrics about chunks (number of messages, size in bytes).
* A `MessageListener`: this callback should contain the application code
to process a message.

The 2 callbacks are provided when creating the `Client` instance:

.Consuming messages
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=consume]
--------
<1> Set chunk listener
<2> Ask for a new chunk after each chunk delivery
<3> Set message listener
<4> Get and process byte array payload

Asking for a new credit on each chunk is usually a good default, so
the `ChunkListener` implementation above can be taken as-is for
most applications.

[NOTE]
.What's the deal with credits?
====
The broker cannot keep sending messages to a client if the latter
cannot keep up. There must be a way for the client and the server
to agree on the delivery rate, not too high, but also not to low
(to keep the client busy). Credits are way to achieve this.

When subscribing, the client specifies an initial number of credits. For
every chunk the broker delivers, it removes a credit from the initial
count. Once the credit count reaches 0 for the subscription, the broker
stops sending chunks of messages. The client needs to ask for more credits to see
more chunks of messages getting sent. This way a client can make
sure it is not overwhelmed by the flow of incoming message.

But the client must be careful in the way it asks for credits.
It can choose to ask for more
chunks when its credit count reaches 0, but this is usually not a good strategy:
during the time the credit request arrives to the broker and a new chunk
is received, the client remains idle, wasting processing time. This is why
it is usually preferable to ask for a reasonable amount of initial credits
(a few, not dozens, remember a chunk can contain thousands of messages)
and ask for a new credit on every chunk.

Inappropriate credit values can be the cause of an unstable consuming rate.
The recommendation given above is usually a good rule of thumb, but may not
be appropriate for all workloads.
====

=== Working with Complex Messages

== Using the Performance Tool