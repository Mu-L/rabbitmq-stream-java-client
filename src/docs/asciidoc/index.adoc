= RabbitMQ Stream Java Client
:revnumber: {project-version}
:example-caption!:
ifndef::imagesdir[:imagesdir: images]
ifndef::sourcedir[:sourcedir: ../../main/java]
:source-highlighter: prettify
:test-examples: ../../test/java/com/rabbitmq/stream/docs

The RabbitMQ Stream Java Client is a Java library to communicate with
the https://github.com/rabbitmq/rabbitmq-stream[RabbitMQ Stream Plugin].
It allows to create and delete streams, as well as to publish to and consume from
these streams.

== What is a RabbitMQ Stream?

A RabbitMQ stream is an append-only, FIFO structure. It differs from the classical
RabbitMQ queue in the way message consumption works. In a classical RabbitMQ queue,
consuming removes messages from the queue. In a RabbitMQ stream, consuming leaves
the stream intact. So the content of a stream can read and re-read without
impact or destructive effect.

None of the stream or classical queue data structure is better than the other,
they are usually suited for different use cases.

== When to Use RabbitMQ Stream?

A stream abstraction is useful when one or several consumer applications
require the whole history of data ("replay").

Streams can also be useful when a higher throughput than with classical
RabbitMQ queues is required. RabbitMQ streams use various optimization techniques
and a custom network protocol to achieve performances that are not possible
with the other protocols supported in RabbitMQ (AMQP, STOMP, MQTT). This
does not make the stream protocol better than these protocols, they just
all serve different purposes.

== Other Way to Use Streams in RabbitMQ

It is also possible to use the stream abstraction in RabbitMQ
with the AMQP 0-9-1 protocol. Instead of consuming from a stream
with the stream protocol, one consumes from a _stream queue_ with
the AMQP 0-9-1. A _stream queue_ is a special type of queue that
has been adapted to provide the stream semantics (mainly non-destructive
reading).

Using a stream queue has the advantage to provide the features
inherent to the stream abstraction (append-only structure, non-destructive
reading) with any AMQP 0-9-1 client library. This is clearly
interesting when considering the maturity of AMQP 0-9-1 client libraries
and the ecosystem around AMQP 0-9-1.

But by using a stream queue, one does not benefit from the performance
of the stream protocol, which has been designed for performance in mind,
whereas AMQP 0-9-1 is a more general-purpose protocol.

It is not possible to use stream queues with the stream Java client,
you need to use an AMQP 0-9-1 client library.

== Guarantees

RabbitMQ stream provides at-least-once guarantees thanks to the
publisher confirm mechanism, which is supported by the stream Java client.

== Using the Stream Java Client

The library requires Java 8 or more.

A RabbitMQ node with the stream plugin enabled is required:

* Download the https://bintray.com/rabbitmq/all-dev/rabbitmq-stream[latest generic Unix alpha from Bintray].
* Follow the https://www.rabbitmq.com/install-generic-unix.html[instructions to install the generic Unix package].
* Enable the plugin `./rabbitmq-plugins enable rabbitmq_stream`.
* Start the broker `./rabbitmq-server -detached`. This starts the stream listener on port 5555.

=== Dependencies

Use your favorite build management tool to add the client dependencies to your project.

Note the client uses the https://github.com/apache/qpid-proton-j[Apache QPid Proton-J]
library for <<working-with-complex-messages,AMQP 1.0 message encoding and decoding>>.

==== Maven

.pom.xml
[source,xml,subs="attributes,specialcharacters"]
----
<dependencies>

  <dependency>
    <groupId>com.rabbitmq</groupId>
    <artifactId>stream-client</artifactId>
    <version>{project-version}</version>
  </dependency>

  <dependency>
    <groupId>org.apache.qpid</groupId>
    <artifactId>proton-j</artifactId>
    <version>{protonj-version}</version>
  </dependency>

</dependencies>

<repositories>

  <repository>
    <id>ossrh</id>
    <url>https://oss.sonatype.org/content/repositories/snapshots</url>
    <snapshots><enabled>true</enabled></snapshots>
    <releases><enabled>false</enabled></releases>
  </repository>

</repositories>
----

==== Gradle

.build.gradle
[source,groovy,subs="attributes,specialcharacters"]
----
dependencies {
  compile "com.rabbitmq:stream-client:{project-version}"
  compile "org.apache.qpid:proton-j:{protonj-version}"
}

repositories {
  maven { url 'https://oss.sonatype.org/content/repositories/snapshots' }
  mavenCentral()
}
----

=== Connecting to RabbitMQ Stream

The main API of the stream Java client is the `com.rabbitmq.stream.Client`
class. To connect to a local instance of the RabbitMQ Stream plugin
with the default user and password, just create an `Client` instance

.Connecting with all the defaults
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=client-creation]
--------
<1> Connect to localhost:5555 with guest / guest

A `Client` constructor can take a `ClientParameters` to set
parameters like host, port, username, and password:

.Connecting with parameters
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=client-creation-with-client-parameters]
--------
<1> Use `ClientParameters` to set connection parameters

=== Managing Streams

Streams can be created with the `Client#create(String)` method:

.Creating a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-creation]
--------
<1> Create the `my-stream` stream
<2> Check if creation is OK
<3> Check response code

The `Constants` class can help to make sense of response codes.

The deletion of a stream follows the same pattern with the
`Client#delete(String)` method:

.Deleting a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-deletion]
--------
<1> Delete the `my-stream` stream
<2> Check if deletion is OK
<3> Check response code

Note you should avoid stream churn (creating and deleting streams
repetitively) as their creation and deletion imply
some significant housekeeping on the server side (interactions
with the file system, communication between nodes of the cluster).
Therefore you should consider streams as long-lived objects.

=== Publishing Messages

The simplest way to publish something to a stream is the
`Client#publish(Stream, byte[])` method:

.Publishing a message
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-simple]
--------
<1> The payload of a message is an array of bytes
<2> The stream to publish to
<3> The content to publish

The `publish` method returns a long: it is the publishing identifier
for the message. This identifier is used to correlate the message
with a publishing confirmation that the broker sends asynchronously.
This allows making sure a message made it to the broker and provides
at-least-once guarantees. Each `Client` instance can register a callback to react
to publish confirms when the instance is created:

.Handling publish confirms
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-confirm-callback]
--------
<1> Set publish confirm callback on client creation

The broker also sends notification for messages that it could not
take into account, for example when the targeted stream does not
exist or is not available. Just like with publish confirms, set
up the listener when creating the client instance:

.Handling publishing errors
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-error-callback]
--------
<1> Set publish error callback on client creation

The `PublishErrorListener` callback provides the publishing ID as well
as an error code to figure out the reason of the publishing error.

NOTE: The publishing identifier scopes the client instance connection, it
is not a global identifier for the message.

It is also possible to publish messages in batch, with the
`publishBinary(List<byte[]>)` method. The method returns a
list of publishing identifiers
to handle publish confirms. The following snippet shows how to use
publish messages in batch:

.Publishing messages in batch
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-multiple]
--------
<1> Messages are a list of byte arrays
<2> The stream to publish to
<3> The messages to publish

TIP: Use batch publishing to improve throughput.

You may wonder if messages are only made of a byte array payload. They are not.
They can have fixed and arbitrary headers. We will see how to
create complex messages in a dedicated section.

=== Consuming Messages

The `Client#subscribe` method allows subscribing to a stream:

.Subscribing to a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=subscribe]
--------
<1> The identifier of the subscription
<2> The stream to consume from
<3> The offset to start consuming from
<4> The initial number of credits
<5> Check if response is OK
<6> Check response code

Once the subscription is done, the broker will start sending messages.
We will see how to handle these messages asynchronously in a callback in a moment.
Let's have a further look at the parameters of `subscribe`:

* Identifier of the subscription: this is an arbitrary number to correlate
the subscription with the inbound messages. The value must be unique within
the connection.
* Stream: this is the stream to consume from. It obviously must exist.
* Offset: this is the offset to start consuming from. 0 means consuming
from the beginning of the stream (first messages). -1 means consuming
from the end of the stream at the time of subscription (last messages). The client
can also specify a number in between, for example the offset where it left off
in a previous incarnation of the application.
* Initial number of credits: the number of chunks of messages the broker can
send immediately after the subscription. This is a way to throttle delivery
in case the broker send messages faster than the client can process them. We will
see more about credits later.

The broker delivers messages in chunks (batches). The size of a chunk is not
fixed, it can vary from a few messages to thousands of them in the same chunk.
Delivering messages in chunks instead of one by one contributes to the higher throughput
RabbitMQ Stream can achieve compared to other protocols like AMQP 0-9-1
(which do use batch at all).

The `Client` provides 2 callbacks to handle inbound messages: one for each
chunk and another one for each message within a chunk. Here is more information
about each callback:

* A `ChunkListener`: this callback is merely technical, it gives
the application the occasion to ask for more chunks of messages
or to compute metrics about chunks (number of messages, size in bytes).
* A `MessageListener`: this callback should contain the application code
to process a message.

The 2 callbacks are provided when creating the `Client` instance:

.Consuming messages
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=consume]
--------
<1> Set chunk listener
<2> Ask for a new chunk after each chunk delivery
<3> Set message listener
<4> Get and process byte array payload

Asking for a new credit on each chunk is usually a good default, so
the `ChunkListener` implementation above can be taken as-is for
most applications.

[NOTE]
.What's the deal with credits?
====
The broker cannot keep sending messages to a client if the latter
cannot keep up. There must be a way for the client and the server
to agree on the delivery rate, not too high, but also not to low
(to keep the client busy). Credits are way to achieve this.

When subscribing, the client specifies an initial number of credits. For
every chunk the broker delivers, it removes a credit from the initial
count. Once the credit count reaches 0 for the subscription, the broker
stops sending chunks of messages. The client needs to ask for more credits to see
more chunks of messages getting sent. This way a client can make
sure it is not overwhelmed by the flow of incoming message.

But the client must be careful in the way it asks for credits.
It can choose to ask for more
chunks when its credit count reaches 0, but this is usually not a good strategy:
during the time the credit request arrives to the broker and a new chunk
is received, the client remains idle, wasting processing time. This is why
it is usually preferable to ask for a reasonable amount of initial credits
(a few, not dozens, remember a chunk can contain thousands of messages)
and ask for a new credit on every chunk.

Inappropriate credit values can be the cause of an unstable consuming rate.
The recommendation given above is usually a good rule of thumb, but may not
be appropriate for all workloads.
====

=== Working with Complex Messages

The publishing and consuming examples showed that messages are made of
a byte array payload, but they did not go much further. Messages in RabbitMQ Stream
can actually be more sophisticated, as they comply to the
https://www.amqp.org/resources/specifications[AMQP 1.0 message format].

In a nutshell, a message in RabbitMQ Stream has the following structure:

* properties: _a defined set of standard properties of the message_ (e.g.
message ID, correlation ID, content type, etc).
* application properties: a set of arbitrary key/value pairs.
* body: typically an array of bytes.
* message annotations: a set of key/value pairs (aimed at the infrastructure).

The RabbitMQ Stream Java client uses the `Message` interface to abstract
a message and the recommended way to create `Message` instances is to
use the `Client#messageBuilder` method. To publish a `Message`, use
the `Client#publish(String,Message)`:

.Creating a `Message` instance
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=message-creation]
--------
<1> Get the message builder from the client
<2> Get the properties builder and set some properties
<3> Go back to message builder
<4> Set byte array payload
<5> Build the message instance
<6> Publish the message

[NOTE]
.Is RabbitMQ Stream based on AMQP 1.0?
====
AMQP 1.0 is a standard that defines _an efficient binary peer-to-peer
protocol for transporting messages between two processes over a network_.
It also defines _an abstract message format, with concrete standard encoding_.
This is only the latter part that RabbitMQ Stream uses. The AMQP 1.0 protocol is not used,
only AMQP 1.0 encoded messages are wrapped into the RabbitMQ Stream binary protocol.

The actual AMQP 1.0 message encoding and decoding happen on the client side, the
RabbitMQ Stream plugin stores only bytes, it has no idea that AMQP 1.0 message format
is used.

AMQP 1.0 message format was chosen because of its flexibility and its advanced
type system. It provides good interoperability, which allows streams
to be accessed as AMQP 0-9-1 queues, without data loss.
====

== Using the Performance Tool

The library contains also a performance tool to test the RabbitMQ Stream plugin.
It is https://bintray.com/rabbitmq/java-tools-dev/stream-perf-test[downloadable from Bintray]
as an uber JAR and can be built separately as well.

To launch a run:

----
$ java -jar stream-perf-test-{version}.jar
10:11:54.324 [main] INFO  c.r.stream.perf.StreamPerfTest - Created stream stream1
10:11:54.385 [main] INFO  c.r.stream.perf.StreamPerfTest - Producer will stream stream1
10:11:54.387 [main] INFO  c.r.stream.perf.StreamPerfTest - Starting consuming on stream1
10:11:54.390 [main] INFO  c.r.stream.perf.StreamPerfTest - Starting producer
1, published 155230 msg/s, confirmed 147824 msg/s, consumed 124487 msg/s, latency min/median/75th/95th/99th 1121/8225/17647/62468/73991 µs, chunk size 109
2, published 359193 msg/s, confirmed 336535 msg/s, consumed 306748 msg/s, latency min/median/75th/95th/99th 1398/56590/80607/127818/135925 µs, chunk size 345
3, published 523429 msg/s, confirmed 509044 msg/s, consumed 478710 msg/s, latency min/median/75th/95th/99th 1478/29996/69536/111946/135079 µs, chunk size 529
4, published 599735 msg/s, confirmed 594707 msg/s, consumed 568315 msg/s, latency min/median/75th/95th/99th 964/21032/52977/98643/133399 µs, chunk size 548
5, published 632114 msg/s, confirmed 609804 msg/s, consumed 591426 msg/s, latency min/median/75th/95th/99th 964/34303/74318/110684/127440 µs, chunk size 588
6, published 619328 msg/s, confirmed 618229 msg/s, consumed 598410 msg/s, latency min/median/75th/95th/99th 964/45918/86391/114714/138207 µs, chunk size 657
^C
Summary: published 641792 msg/s, confirmed 635240 msg/s, consumed 636256 msg/s, latency 95th 112730 µs, chunk size 711
----

The previous command will start publishing to and consuming from a stream created
only for the test. The tool outputs live metrics on the console and write more
detailed metrics in a `stream-perf-test-current.txt` file that get renamed to
`stream-perf-test-yyyy-MM-dd-HHmmss.txt` when the run ends.

To see the options:

----
java -jar stream-perf-test-{version}.jar --help
----

=== Building the Performance Tool

To build the uber JAR:

----
./mvnw clean package -Dmaven.test.skip -P performance-tool
----

Then run the tool:

----
java -jar target/stream-perf-test.jar
----