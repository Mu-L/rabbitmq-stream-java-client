= RabbitMQ Stream Java Client
:revnumber: {project-version}
:example-caption!:
ifndef::imagesdir[:imagesdir: images]
ifndef::sourcedir[:sourcedir: ../../main/java]
:source-highlighter: prettify
:test-examples: ../../test/java/com/rabbitmq/stream/docs

The RabbitMQ Stream Java Client is a Java library to communicate with
the https://github.com/rabbitmq/rabbitmq-stream[RabbitMQ Stream Plugin].
It allows to create and delete streams, as well as to publish to and consume from
these streams.

== What is a RabbitMQ Stream?

A RabbitMQ stream is an append-only, FIFO structure. It differs from the classical
RabbitMQ queue in the way message consumption works. In a classical RabbitMQ queue,
consuming removes messages from the queue. In a RabbitMQ stream, consuming leaves
the stream intact. So the content of a stream can read and re-read without
impact or destructive effect.

None of the stream or classical queue data structure is better than the other,
they are usually suited for different use cases.

== When to Use RabbitMQ Stream?

A stream abstraction is useful when one or several consumer applications
require the whole history of data ("replay").

Streams can also be useful when a higher throughput than with classical
RabbitMQ queues is required. RabbitMQ streams use various optimization techniques
and a custom network protocol to achieve performances that are not possible
with the other protocols supported in RabbitMQ (AMQP, STOMP, MQTT). This
does not make the stream protocol better than these protocols, they just
all serve different purposes.

== Other Way to Use Streams in RabbitMQ

It is also possible to use the stream abstraction in RabbitMQ
with the AMQP 0-9-1 protocol. Instead of consuming from a stream
with the stream protocol, one consumes from a _stream queue_ with
the AMQP 0-9-1. A _stream queue_ is a special type of queue that
has been adapted to provide the stream semantics (mainly non-destructive
reading).

Using a stream queue has the advantage to provide the features
inherent to the stream abstraction (append-only structure, non-destructive
reading) with any AMQP 0-9-1 client library. This is clearly
interesting when considering the maturity of AMQP 0-9-1 client libraries
and the ecosystem around AMQP 0-9-1.

But by using a stream queue, one does not benefit from the performance
of the stream protocol, which has been designed for performance in mind,
whereas AMQP 0-9-1 is a more general-purpose protocol.

It is not possible to use stream queues with the stream Java client,
you need to use an AMQP 0-9-1 client library.

== Guarantees

RabbitMQ stream provides at-least-once guarantees thanks to the
publisher confirm mechanism, which is supported by the stream Java client.

== The Stream Java Client

The library requires Java 8 or more.

=== Setting up RabbitMQ

A RabbitMQ node with the stream plugin enabled is required. The easiest way
to get up and running is to use Docker. It is also possible to use the
generic Unix package.

==== With Docker

The following command creates a one-time Docker container to run RabbitMQ
with the stream plugin enabled:

.Running the stream plugin with Docker
----
docker run -it --rm --name rabbitmq -p 5555:5555 pivotalrabbitmq/rabbitmq-stream
----

The previous command exposes only the stream port (5555), you can expose
ports for other protocols:

.Exposing the AMQP 0.9.1 and management ports:
----
docker run -it --rm --name rabbitmq -p 5555:5555 -p 5672:5672 -p 15672:15672 \
    pivotalrabbitmq/rabbitmq-stream
----

Refer to the official https://hub.docker.com/_/rabbitmq[RabbitMQ Docker image web page]
to find out more about its usage. Make sure to use the `pivotalrabbitmq/rabbitmq-stream`
image in the command line.

The `pivotalrabbitmq/rabbitmq-stream` Docker image is meant for development usage only. It does not
support all the features of the official Docker image, like TLS.

==== With the Generic Unix Package

The generic Unix package requires https://www.rabbitmq.com/which-erlang.html[Erlang] to be installed.

* Download the https://bintray.com/rabbitmq/all-dev/rabbitmq-stream[latest generic Unix alpha from Bintray].
* Follow the https://www.rabbitmq.com/install-generic-unix.html[instructions to install the generic Unix package].
* Enable the plugin `./rabbitmq-plugins enable rabbitmq_stream`.
* Start the broker `./rabbitmq-server -detached`. This starts the stream listener on port 5555.

=== Dependencies

Use your favorite build management tool to add the client dependencies to your project.

Note the client uses the https://github.com/apache/qpid-proton-j[Apache QPid Proton-J]
library for <<working-with-complex-messages,AMQP 1.0 message encoding and decoding>>.

==== Maven

.pom.xml
[source,xml,subs="attributes,specialcharacters"]
----
<dependencies>

  <dependency>
    <groupId>com.rabbitmq</groupId>
    <artifactId>stream-client</artifactId>
    <version>{project-version}</version>
  </dependency>

  <dependency>
    <groupId>org.apache.qpid</groupId>
    <artifactId>proton-j</artifactId>
    <version>{protonj-version}</version>
  </dependency>

</dependencies>

<repositories>

  <repository>
    <id>ossrh</id>
    <url>https://oss.sonatype.org/content/repositories/snapshots</url>
    <snapshots><enabled>true</enabled></snapshots>
    <releases><enabled>false</enabled></releases>
  </repository>

</repositories>
----

==== Gradle

.build.gradle
[source,groovy,subs="attributes,specialcharacters"]
----
dependencies {
  compile "com.rabbitmq:stream-client:{project-version}"
  compile "org.apache.qpid:proton-j:{protonj-version}"
}

repositories {
  maven { url 'https://oss.sonatype.org/content/repositories/snapshots' }
  mavenCentral()
}
----

=== Sample Application

This section covers the basics of the stream Java client by building
a small publish/consume application. This is a good way to get
an overview of the client. If you want a more comprehensive introduction,
you can go to the <<client-reference,the reference documentation section>>.

The sample application publishes some messages and then registers
a consumer to make some computations out of them. The
https://github.com/rabbitmq/rabbitmq-stream-java-client/blob/master/src/test/java/com/rabbitmq/stream/docs/SampleApplication.java[source code is available on GitHub].

The sample class starts with a few imports:

.Imports for the sample application
[source,java,indent=0]
--------
include::{test-examples}/SampleApplication.java[tag=sample-imports]
--------

Then comes the publishing part. The next snippet shows a client instance
used to publish messages. The body of each message is made of the value
of a sequence. A count down latch is used to make sure the broker confirmed
all the messages before closing the publisher connection.

.Publishing messages
[source,java,indent=0]
--------
include::{test-examples}/SampleApplication.java[tag=sample-publisher]
--------
<1> Count down on message publishing confirmation
<2> Create the target stream
<3> Publish messages in a loop
<4> Wait for all publishing confirmations to have arrived
<5> Close the publisher connection

The next snippet shows how to consume and process the messages published previously.
It sets up a chunk listener when creating the client instance. The chunk listener is called
every time the broker delivers a chunk (batch) of messages to the client. It
asks the broker to deliver another chunk with the `credit` command. A message listener
is also set up on client creation. The message listener is called every time a message
is delivered (as part of a chunk). Once all messages have been received, the sum is output
to the console, the stream is deleted, and the consumer connection is closed.

.Consuming messages
[source,java,indent=0]
--------
include::{test-examples}/SampleApplication.java[tag=sample-consumer]
--------
<1> Ask for a new chunk for each chunk received
<2> Add the value in the message body to the sum
<3> Count down on each message
<4> Start consuming at the beginning of the stream
<5> Wait for all messages to have arrived
<6> Output the sum
<7> Delete the stream
<8> Close the consumer connection

You can run the sample application from the root of the project (you need
a running local RabbitMQ node with the stream plugin enabled):

----
$ ./mvnw -q test-compile exec:java -Dexec.classpathScope="test" \
    -Dexec.mainClass="com.rabbitmq.stream.docs.SampleApplication"
Starting publishing...
Published 10000 messages
Starting consuming...
Sum: 49995000
----

You can remove the `-q` flag if you want more insight on the execution of the build.

=== Client Reference

==== Connecting to RabbitMQ Stream

The main API of the stream Java client is the `com.rabbitmq.stream.Client`
class. To connect to a local instance of the RabbitMQ Stream plugin
with the default user and password, just create an `Client` instance

.Connecting with all the defaults
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=client-creation]
--------
<1> Connect to localhost:5555 with guest / guest

A `Client` constructor can take a `ClientParameters` to set
parameters like host, port, virtual host, username, and password:

.Connecting with parameters
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=client-creation-with-client-parameters]
--------
<1> Use `ClientParameters` to set connection parameters

==== Managing Streams

Streams can be created with the `Client#create(String)` method:

.Creating a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-creation]
--------
<1> Create the `my-stream` stream
<2> Check if creation is OK
<3> Check response code

The `Constants` class can help to make sense of response codes.

The deletion of a stream follows the same pattern with the
`Client#delete(String)` method:

.Deleting a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-deletion]
--------
<1> Delete the `my-stream` stream
<2> Check if deletion is OK
<3> Check response code

Note you should avoid stream churn (creating and deleting streams
repetitively) as their creation and deletion imply
some significant housekeeping on the server side (interactions
with the file system, communication between nodes of the cluster).
Therefore you should consider streams as long-lived objects.

==== Limiting the Size of a Stream

A stream is an append-only data structure and reading from
it does not remove data. This means a stream can grow indefinitely. RabbitMQ Stream
supports a size-based retention policy: once
the stream reaches a given size, it is truncated (starting from the beginning).

[IMPORTANT]
.Limit the size of streams if appropriate!
====
Make sure to set up a retention policy on potentially large streams
if you don't want to saturate the storage devices of your servers. Keep
in mind that this means some data will be erased!
====

It is possible to set up the retention policy when creating the stream:

.Setting the retention policy when creating a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-creation-retention]
--------
<1> Set the maximum size to 10 GB
<2> Set the segment size to 500 MB

The previous snippet mentions a segment size. RabbitMQ Stream does not store
a stream in a big, single file, it uses segment files for technical reasons.
A stream is truncated by deleting whole segment files (and not part of them),
so the maximum size of a stream is usually significantly higher than the size
of segment files. 500 MB is a reasonable segment file size to begin with.

You can also use the `ByteCapacity` class to set up the retention policy,
it is just a matter of API preference:

.Using `ByteCapacity` to set up the retention policy
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=stream-creation-retention-alternative]
--------
<1> Set the maximum size to 10 GB
<2> Set the segment size to 500 MB


==== Publishing Messages

The simplest way to publish something to a stream is the
`Client#publish(Stream, byte[])` method:

.Publishing a message
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-simple]
--------
<1> The payload of a message is an array of bytes
<2> The stream to publish to
<3> The content to publish

The `publish` method returns a long: it is the publishing identifier
for the message. This identifier is used to correlate the message
with a publishing confirmation that the broker sends asynchronously.
This allows making sure a message made it to the broker and provides
at-least-once guarantees.

NOTE: The publishing identifier is local to the client instance connection. It
is neither a global identifier for the message, nor the offset of the message
in the stream.

Each `Client` instance can register a callback to react
to publish confirms when the instance is created:

.Handling publish confirms
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-confirm-callback]
--------
<1> Set publish confirm callback on client creation

The broker also sends notification for messages that it could not
take into account, for example when the targeted stream does not
exist or is not available. Just like with publish confirms, set
up the listener when creating the client instance:

.Handling publishing errors
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-error-callback]
--------
<1> Set publish error callback on client creation

The `PublishErrorListener` callback provides the publishing ID as well
as an error code to figure out the reason of the publishing error.

It is also possible to publish messages in batch, with the
`publishBinary(List<byte[]>)` method. The method returns a
list of publishing identifiers
to handle publish confirms. The following snippet shows how to use
publish messages in batch:

.Publishing messages in batch
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=publish-multiple]
--------
<1> Messages are a list of byte arrays
<2> The stream to publish to
<3> The messages to publish

TIP: Use batch publishing to improve throughput.

You may wonder if messages are only made of a byte array payload. They are not.
They can have fixed and arbitrary headers. We will see how to
create complex messages in a dedicated section.

==== Consuming Messages

The `Client#subscribe` method allows subscribing to a stream:

.Subscribing to a stream
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=subscribe]
--------
<1> The identifier of the subscription
<2> The stream to consume from
<3> The offset to start consuming from
<4> The initial number of credits
<5> Check if response is OK
<6> Check response code

Once the subscription is done, the broker will start sending messages.
We will see how to handle these messages asynchronously in a callback in a moment.
Let's have a further look at the parameters of `subscribe`:

* Identifier of the subscription: this is an arbitrary number to correlate
the subscription with the inbound messages. The value must be unique within
the connection.
* Stream: this is the stream to consume from. It obviously must exist.
* Offset: this is the offset to start consuming from. There are several ways to
specify the offset:
** `OffsetSpecification.first()`: starting from the first available offset. If
the stream has not been <<limiting-the-size-of-a-stream,truncated>>, this
means the beginning of the stream (offset 0).
** `OffsetSpecification.last()`: starting from the end of the stream and returning
the last chunk of messages immediately (if the stream is not empty).
** `OffsetSpecification.next()`: starting from the next offset to be written. Contrary
to `OffsetSpecification.last()`, consuming with `OffsetSpecification.next()`
will not return anything if no-one is publishing to the stream. The broker will start
sending messages to the consumer when messages are published to the stream.
** `OffsetSpecification.offset(offset)`: starting from the specified offset. 0 means consuming
from the beginning of the stream (first messages). The client
can also specify any number, for example the offset where it left off
in a previous incarnation of the application.
** `OffsetSpecification.timestamp(timestamp)`: starting from the messages stored
after the specified timestamp.
* Initial number of credits: the number of chunks of messages the broker can
send immediately after the subscription. This is a way to throttle delivery
in case the broker send messages faster than the client can process them. We will
see more about credits later.

The broker delivers messages in chunks (batches). The size of a chunk is not
fixed, it can vary from a few messages to thousands of them in the same chunk.
Delivering messages in chunks instead of one by one contributes to the higher throughput
RabbitMQ Stream can achieve compared to other protocols like AMQP 0-9-1
(which do use batch at all).

The `Client` provides 2 callbacks to handle inbound messages: one for each
chunk and another one for each message within a chunk. Here is more information
about each callback:

* A `ChunkListener`: this callback is merely technical, it gives
the application the occasion to ask for more chunks of messages
or to compute metrics about chunks (number of messages, size in bytes).
* A `MessageListener`: this callback should contain the application code
to process a message.

The 2 callbacks are provided when creating the `Client` instance:

.Consuming messages
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=consume]
--------
<1> Set chunk listener
<2> Ask for a new chunk after each chunk delivery
<3> Set message listener
<4> Get and process byte array payload

Asking for a new credit on each chunk is usually a good default, so
the `ChunkListener` implementation above can be taken as-is for
most applications.

[NOTE]
.What's the deal with credits?
====
The broker cannot keep sending messages to a client if the latter
cannot keep up. There must be a way for the client and the server
to agree on the delivery rate, not too high, but also not to low
(to keep the client busy). Credits are way to achieve this.

When subscribing, the client specifies an initial number of credits. For
every chunk the broker delivers, it removes a credit from the initial
count. Once the credit count reaches 0 for the subscription, the broker
stops sending chunks of messages. The client needs to ask for more credits to see
more chunks of messages getting sent. This way a client can make
sure it is not overwhelmed by the flow of incoming message.

But the client must be careful in the way it asks for credits.
It can choose to ask for more
chunks when its credit count reaches 0, but this is usually not a good strategy:
during the time the credit request arrives to the broker and a new chunk
is received, the client remains idle, wasting processing time. This is why
it is usually preferable to ask for a reasonable amount of initial credits
(a few, not dozens, remember a chunk can contain thousands of messages)
and ask for a new credit on every chunk.

Inappropriate credit values can be the cause of an unstable consuming rate.
The recommendation given above is usually a good rule of thumb, but may not
be appropriate for all workloads.
====

==== Working with Complex Messages

The publishing and consuming examples showed that messages are made of
a byte array payload, but they did not go much further. Messages in RabbitMQ Stream
can actually be more sophisticated, as they comply to the
https://www.amqp.org/resources/specifications[AMQP 1.0 message format].

In a nutshell, a message in RabbitMQ Stream has the following structure:

* properties: _a defined set of standard properties of the message_ (e.g.
message ID, correlation ID, content type, etc).
* application properties: a set of arbitrary key/value pairs.
* body: typically an array of bytes.
* message annotations: a set of key/value pairs (aimed at the infrastructure).

The RabbitMQ Stream Java client uses the `Message` interface to abstract
a message and the recommended way to create `Message` instances is to
use the `Client#messageBuilder` method. To publish a `Message`, use
the `Client#publish(String,Message)`:

.Creating a `Message` instance
[source,java,indent=0]
--------
include::{test-examples}/ClientUsage.java[tag=message-creation]
--------
<1> Get the message builder from the client
<2> Get the properties builder and set some properties
<3> Go back to message builder
<4> Set byte array payload
<5> Build the message instance
<6> Publish the message

[NOTE]
.Is RabbitMQ Stream based on AMQP 1.0?
====
AMQP 1.0 is a standard that defines _an efficient binary peer-to-peer
protocol for transporting messages between two processes over a network_.
It also defines _an abstract message format, with concrete standard encoding_.
This is only the latter part that RabbitMQ Stream uses. The AMQP 1.0 protocol is not used,
only AMQP 1.0 encoded messages are wrapped into the RabbitMQ Stream binary protocol.

The actual AMQP 1.0 message encoding and decoding happen on the client side, the
RabbitMQ Stream plugin stores only bytes, it has no idea that AMQP 1.0 message format
is used.

AMQP 1.0 message format was chosen because of its flexibility and its advanced
type system. It provides good interoperability, which allows streams
to be accessed as AMQP 0-9-1 queues, without data loss.
====

=== Builing the Client

You need JDK 1.8 or more installed.

To build the JAR file:

----
./mvnw clean package -DskipITs -DskipTests
----

To launch the test suite (requires a local RabbitMQ node with stream plugin enabled):

----
./mvnw verify
----

== The Performance Tool

The library contains also a performance tool to test the RabbitMQ Stream plugin.
It is https://bintray.com/rabbitmq/java-tools-dev/stream-perf-test[downloadable from Bintray]
as an uber JAR and can be built separately as well.

=== Using the Performance Tool

To launch a run:

----
$ java -jar stream-perf-test-{version}.jar
10:11:54.324 [main] INFO  c.r.stream.perf.StreamPerfTest - Created stream stream1
10:11:54.385 [main] INFO  c.r.stream.perf.StreamPerfTest - Producer will stream stream1
10:11:54.387 [main] INFO  c.r.stream.perf.StreamPerfTest - Starting consuming on stream1
10:11:54.390 [main] INFO  c.r.stream.perf.StreamPerfTest - Starting producer
1, published 155230 msg/s, confirmed 147824 msg/s, consumed 124487 msg/s, latency min/median/75th/95th/99th 1121/8225/17647/62468/73991 µs, chunk size 109
2, published 359193 msg/s, confirmed 336535 msg/s, consumed 306748 msg/s, latency min/median/75th/95th/99th 1398/56590/80607/127818/135925 µs, chunk size 345
3, published 523429 msg/s, confirmed 509044 msg/s, consumed 478710 msg/s, latency min/median/75th/95th/99th 1478/29996/69536/111946/135079 µs, chunk size 529
4, published 599735 msg/s, confirmed 594707 msg/s, consumed 568315 msg/s, latency min/median/75th/95th/99th 964/21032/52977/98643/133399 µs, chunk size 548
5, published 632114 msg/s, confirmed 609804 msg/s, consumed 591426 msg/s, latency min/median/75th/95th/99th 964/34303/74318/110684/127440 µs, chunk size 588
6, published 619328 msg/s, confirmed 618229 msg/s, consumed 598410 msg/s, latency min/median/75th/95th/99th 964/45918/86391/114714/138207 µs, chunk size 657
^C
Summary: published 641792 msg/s, confirmed 635240 msg/s, consumed 636256 msg/s, latency 95th 112730 µs, chunk size 711
----

The previous command will start publishing to and consuming from a stream created
only for the test. The tool outputs live metrics on the console and write more
detailed metrics in a `stream-perf-test-current.txt` file that get renamed to
`stream-perf-test-yyyy-MM-dd-HHmmss.txt` when the run ends.

To see the options:

----
java -jar stream-perf-test-{version}.jar --help
----

The performance tool comes also with a completion script. You can download it and enable it in
your `~/.zshrc` file:

----
alias stream-perf-test='java -jar target/stream-perf-test.jar'
source ~/.zsh/stream-perf-test_completion
----

Note the activation requires an alias which must be `stream-perf-test`. The command can be anything
though.

=== Building the Performance Tool

To build the uber JAR:

----
./mvnw clean package -Dmaven.test.skip -P performance-tool
----

Then run the tool:

----
java -jar target/stream-perf-test.jar
----