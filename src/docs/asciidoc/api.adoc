:test-examples: ../../test/java/com/rabbitmq/stream/docs

[[rabbitmq-stream-java-api]]
=== RabbitMQ Stream Java API

==== Overview

This section describes the API to connect to the RabbitMQ Stream Plugin, publish messages, and
consume messages. There are 3 main interfaces:

* `com.rabbitmq.stream.Environment` for connecting to a node and optionally
managing streams.
* `com.rabbitmq.stream.Producer` to publish messages.
* `com.rabbitmq.stream.Consumer` to consume messages.

==== Environment

===== Creating the Environment

The environment is the main entry point to a node or a cluster of nodes. `Producer` and
`Consumer` instances are created from an `Environment` instance. Here is the simplest
way to create an `Environment` instance:

.Creating an environment with all the defaults
[source,java,indent=0]
--------
include::{test-examples}/EnvironmentUsage.java[tag=environment-creation]
--------
<1> Create an environment that will connect to localhost:5555
<2> Close the environment after usage

Note the environment must be closed to release resources when it is no
longer needed.

Consider the environment like a long-lived object. An application will usually
create one `Environment` instance when it starts up and close it when it exits.

It is possible to use a URI to specify all the necessary information to
connect to a node:

.Creating an environment with a URI
[source,java,indent=0]
--------
include::{test-examples}/EnvironmentUsage.java[tag=environment-creation-with-uri]
--------
<1> Use the `uri` method to specify the URI to connect to

The previous snippet uses a URI that specifies the following information: host, port,
username, password, and virtual host (`/`, which is encoded as `%2f`).
The URI follows the same rules as the
https://www.rabbitmq.com/uri-spec.html[AMQP 0.9.1 URI],
except the protocol must be `rabbitmq-stream` and TLS is not supported.

When using one URI, the corresponding node will be the main entry point to connect to. The
`Environment` will then use the stream protocol to find out more about streams topology
(leaders and replicas) when asked to create `Producer` and `Consumer` instances. The `Environment`
may become blind if this node goes down though, so it may be more appropriate to specify
several other URIs to try in case of failure of a node:

.Creating an environment with several URIs
[source,java,indent=0]
--------
include::{test-examples}/EnvironmentUsage.java[tag=environment-creation-with-uris]
--------
<1> Use the `uris` method to specify several URIs

By specifying several URIs, the environment will try to connect to the first one, and
will pick a new URI randomly in case of disconnection.

The following table sums up the main settings to create an `Environment`:

[%header,cols=3*]
|===
|Parameter Name
|Description
|Default

|`uri`
|The URI of the node to connect to (single node).
|`rabbitmq-stream://guest:guest@localhost:5555/%2f`

|`uris`
|The URI of the nodes to try to connect to (cluster).
|`rabbitmq-stream://guest:guest@localhost:5555/%2f` singleton list

|`host`
|Host to connect to.
|`localhost`

|`port`
|Port to use.
|`5555`

|`username`
|Username to use to connect.
|`guest`

|`password`
|Password to use to connect.
|`guest`

|`virtualHost`
|Virtual host to connect to.
|`/`

|`recoveryBackOffDelayPolicy`
|Delay policy to use for backoff on connection recovery.
|Fixed delay of 5 seconds

|`topologyUpdateBackOffDelayPolicy`
|Delay policy to use for backoff on topology update, e.g.
when a stream replica moves and a consumer needs to connect to another
node.
|Initial delay of 5 seconds then delay of 1 second.

|`scheduledExecutorService`
|Executor used to schedule infrastructure tasks like background publishing, producers
and consumers migration after disconnection or topology update. If a custom executor is provided,
it is the developer's responsibility to close it once it is no longer necessary.
a|
[source%autofit,java]
----
Executors
  .newScheduledThreadPool(
    Runtime
      .getRuntime()
      .availableProcessors()
);
----
|`maxProducersByConnection`
|The maximum number of `Producer` instances a single connection can maintain before
a new connection is open. The value must be between 1 and 255.
|255

|`maxCommittingConsumersByConnection`
|The maximum number of `Consumer` instances that commit their offset a single connection
can maintain before a new connection is open. The value must be between 1 and 255.
|50

|`maxConsumersByConnection`
|The maximum number of `Consumer` instances a single connection can maintain before
a new connection is open. The value must be between 1 and 255.
|255
|===


===== Managing Streams

Streams are usually long-lived, centrally-managed entities, that is, applications
are not supposed to create and delete them. It is nevertheless possible to create and
delete stream with the `Environment`. This comes in handy for development and testing
purposes.

Streams are created with the `Environment#streamCreator()` method:

.Creating a stream
[source,java,indent=0]
--------
include::{test-examples}/EnvironmentUsage.java[tag=stream-creation]
--------
<1> Create the `my-stream` stream

Streams can be deleted with the `Environment#delete(String)` method:

.Deleting a stream
[source,java,indent=0]
--------
include::{test-examples}/EnvironmentUsage.java[tag=stream-deletion]
--------
<1> Delete the `my-stream` stream

Note you should avoid stream churn (creating and deleting streams repetitively)
as their creation and deletion imply some significant housekeeping on
the server side (interactions with the file system, communication between nodes of the cluster).

[[limiting-the-size-of-a-stream]]It is also possible to limit the size of a stream
when creating it. A stream
is an append-only data structure and reading from it does not remove data.
This means a stream can grow indefinitely. RabbitMQ Stream supports a
size-based and time-based retention policies: once the stream reaches a given size
or a given age, it is truncated (starting from the beginning).

[IMPORTANT]
.Limit the size of streams if appropriate!
====
Make sure to set up a retention policy on potentially large streams
if you don't want to saturate the storage devices of your servers. Keep
in mind that this means some data will be erased!
====

It is possible to set up the retention policy when creating the stream:

.Setting the retention policy when creating a stream
[source,java,indent=0]
--------
include::{test-examples}/EnvironmentUsage.java[tag=stream-creation-retention]
--------
<1> Set the maximum size to 10 GB
<2> Set the segment size to 500 MB

The previous snippet mentions a segment size. RabbitMQ Stream does not store a stream
in a big, single file, it uses segment files for technical reasons.
A stream is truncated by deleting whole segment files (and not part of them)so
the maximum size of a stream is usually significantly higher than the size of
segment files. 500 MB is a reasonable segment file size to begin with.

[NOTE]
.When does the broker enforce the retention policy?
====
The broker enforces the retention policy when the segments of a stream
roll over, that is when the current segment has reached its maximum
size and is closed in favor of a new one. This means the maximum segment
size is a critical setting in the retention mechanism.
====

RabbitMQ Stream also supports a time-based retention policy: segments get truncated
when they reach a certain age. The following snippet
illustrates how to set the time-based retention policy:

.Setting a time-based retention policy when creating a stream
[source,java,indent=0]
--------
include::{test-examples}/EnvironmentUsage.java[tag=stream-creation-time-based-retention]
--------
<1> Set the maximum age to 6 hours
<2> Set the segment size to 500 MB

==== Producer

===== Creating a Producer

A `Producer` instance is created from the `Environment`. The only mandatory
setting to specify is the stream to publish to:

.Creating a producer from the environment
[source,java,indent=0]
--------
include::{test-examples}/ProducerUsage.java[tag=producer-creation]
--------
<1> Use `Environment#producerBuilder()` to define the producer
<2> Specify the stream to publish to
<3> Create the producer instance with `build()`
<4> Close the producer after usage

Consider a `Producer` instance like a long-lived object, do not create one
to send just one message.

Internally, the `Environment` will query the broker to find out about
the topology of the stream and will create or re-use a connection to
publish to the leader node of the stream.

The following table sums up the main settings to create a `Producer`:

[%header,cols=3*]
|===
|Parameter Name
|Description
|Default

|`name`
|The logical name of the producer. Specify a name to enable
<<outbound-message-de-deduplication,message de-deduplication>>.
|`null` (no de-duplication)

|`batchSize`
|The maximum number of messages to accumulate before sending them to the broker.
|100

|`subEntrySize`
|[[producer-sub-entry-size-configuration-entry]]The number of messages to put in a sub-entry. A sub-entry is one "slot" in a publishing
frame, meaning outbound messages are not only batched in publishing frames, but in sub-entries
as well. Use this feature to increase throughput at the cost of increased latency and
potential duplicated messages even when de-duplication is enabled.
|1 (meaning no use of sub-entry batching)

|`maxUnconfirmedMessages`
|The maximum number of unconfirmed outbound messages. `Producer#send` will start
blocking when the limit is reached.
|10,000

|`batchPublishingDelay`
|Period to send a batch of messages.
|100 ms

|`confirmTimeout`
|[[producer-confirm-timeout-configuration-entry]]Time before the client calls the confirm callback to signal
outstanding unconfirmed messages timed out.
|30 seconds

|`enqueueTimeout`
|Time before enqueueing of a message fail when the maximum number of unconfirmed
is reached. The callback of the message will be called with a negative status.
Set the value to `Duration.ZERO` if there should be no timeout.
|10 seconds.
|===

===== Sending Messages

Once a `Producer` has been created, it is possible to send a message with
the `Producer#send(Message, ConfirmationHandler)` method. The following
snippet shows how to publish a message with a byte array payload:

.Sending a message
[source,java,indent=0]
--------
include::{test-examples}/ProducerUsage.java[tag=producer-publish]
--------
<1> The payload of a message is an array of bytes
<2> Create the message with `Producer#messageBuilder()`
<3> Define the behavior on publish confirmation

Messages are not only made of a `byte[]` payload, we will see in
<<working-with-complex-messages,the next section>>
they can also carry pre-defined and application properties.

[NOTE]
.Use a `MessageBuilder` instance only once
====
A `MessageBuilder` instance is meant to create only one message. You
need to create a new instance of `MessageBuilder` for every message
you want to create.
====

The `ConfirmationHandler` defines an asynchronous callback invoked
when the client received from the broker the confirmation the message
has been taken into account. The `ConfirmationHandler` is the place
for any logic on publishing confirmation, including
re-publishing the message if it is negatively acknowledged.

[[working-with-complex-messages]]
===== Working with Complex Messages

The publishing example above showed that messages are made of
a byte array payload, but it did not go much further. Messages in RabbitMQ Stream
can actually be more sophisticated, as they comply to the
https://www.amqp.org/resources/specifications[AMQP 1.0 message format].

In a nutshell, a message in RabbitMQ Stream has the following structure:

* properties: _a defined set of standard properties of the message_ (e.g.
message ID, correlation ID, content type, etc).
* application properties: a set of arbitrary key/value pairs.
* body: typically an array of bytes.
* message annotations: a set of key/value pairs (aimed at the infrastructure).

The RabbitMQ Stream Java client uses the `Message` interface to abstract
a message and the recommended way to create `Message` instances is to
use the `Producer#messageBuilder()` method. To publish a `Message`, use
the `Producer#send(Message,ConfirmationHandler)`:

.Creating a message with properties
[source,java,indent=0]
--------
include::{test-examples}/ProducerUsage.java[tag=producer-publish-complex-message]
--------
<1> Get the message builder from the producer
<2> Get the properties builder and set some properties
<3> Go back to message builder
<4> Set byte array payload
<5> Build the message instance
<6> Publish the message

[NOTE]
.Is RabbitMQ Stream based on AMQP 1.0?
====
AMQP 1.0 is a standard that defines _an efficient binary peer-to-peer
protocol for transporting messages between two processes over a network_.
It also defines _an abstract message format, with concrete standard encoding_.
This is only the latter part that RabbitMQ Stream uses. The AMQP 1.0 protocol is not used,
only AMQP 1.0 encoded messages are wrapped into the RabbitMQ Stream binary protocol.

The actual AMQP 1.0 message encoding and decoding happen on the client side, the
RabbitMQ Stream plugin stores only bytes, it has no idea that AMQP 1.0 message format
is used.

AMQP 1.0 message format was chosen because of its flexibility and its advanced
type system. It provides good interoperability, which allows streams
to be accessed as AMQP 0-9-1 queues, without data loss.
====

[[outbound-message-de-deduplication]]
===== Message De-deduplication

RabbitMQ Stream provides publisher confirms to avoid losing messages: once
the broker has persisted a message it sends a confirmation for this message.
But this can lead to duplicate messages: imagine the connection closes
because of a network glitch after the message has been persisted but _before_
the confirmation reaches the producer. Once reconnected, the producer will
retry to send the same message, as it never received the confirmation. So the
message will be persisted twice.

Luckily RabbitMQ Stream can detect and filter out duplicated messages, based
on 2 client-side elements: the _producer name_ and the _message publishing ID_.

[WARNING]
.De-duplication is not guaranteed when using sub-entries batching
====
It is not possible to guarantee de-duplication when
<<producer-sub-entry-size-configuration-entry, sub-entry batching>> is in use.
Sub-entry batching is disabled by default and it does not prevent from
batching messages in a single publish frame, which can already provide
very high throughput.
====

====== Setting the Name of a Producer

The producer name is set when creating the producer instance, which automatically
enables de-duplication:

.Naming a producer to enable message de-duplication
[source,java,indent=0]
--------
include::{test-examples}/ProducerUsage.java[tag=producer-with-name]
--------
<1> Set a name for the producer
<2> Disable confirm timeout check

Thanks to the name, the broker will be able to track the messages it has persisted
on a given stream for this producer. If the producer connection unexpectedly closes, it
will automatically recover and retry outstanding messages. The broker will then
filter out messages it has already received and persisted. No more duplicates!

[IMPORTANT]
.Why setting `confirmTimeout` to 0 when using de-duplication?
====
The point of de-duplication is to avoid duplicates when retrying unconfirmed messages.
But why retrying in the first place? To avoid _losing_ messages, that is enforcing
_at-least-once_ semantics. If the client does not stubbornly retry messages and gives
up at some point, messages can be lost, which maps to _at-most-once_ semantics. This
is why the de-duplication examples set the
<<producer-confirm-timeout-configuration-entry,`confirmTimeout` setting>> to `Duration.ZERO`:
to disable the background task that calls the confirmation callback for outstanding
messages that time out. This way the client will do its best to retry messages
until they are confirmed.
====

Consider the producer name a logical name. It should not be a random sequence that
changes when the producer application is restarted. Names like `online-shop-order` or
`online-shop-invoice` are better names than `3d235e79-047a-46a6-8c80-9d159d3e1b05`.
There should be only one living instance of a producer with a given name on a given
stream at the same time.

====== Understanding Publishing ID

The producer name is only one part of the de-duplication mechanism, the other part
is the _message publishing ID_. If the producer has a name, the client automatically
assigns a publishing ID to each outbound message for the producer. The publishing ID
is a monotonic sequence, starting at 0 and incremented for each message. The default
publishing sequence is good enough for de-duplication, but it is possible to
assign a publishing ID to each message:

.Using an explicit publishing ID
[source,java,indent=0]
--------
include::{test-examples}/ProducerUsage.java[tag=message-with-publishing-id]
--------
<1> Set a publishing ID on a message

There are a few rules to follow when using a custom publishing ID sequence:

 * the sequence should start at 0
 * the sequence must be monotonic, that is always increments
 * there can be gaps in the sequence (e.g. 0, 1, 2, 3, 6, 7, 9, 10, etc)

A custom publishing ID sequence has usually a meaning: it can be the line number of a file
or the primary key in a database.

Note the publishing ID is not part of the message: it is not stored with the message
and so is not available when consuming the message. It is still possible to store
the value in the AMQP 1.0 message application properties or in an appropriate
properties (e.g. `messageId`).

[IMPORTANT]
.Do not mix client-assigned and custom publishing ID
====
As soon as a producer name is set, message de-duplication is enabled.
It is then possible to let the producer assign a publishing ID to each
message or assign custom publishing IDs. *Do one or the other, not both!*
====

====== Restarting a Producer Where It Left Off

Using a custom publishing sequence is even more useful to restart a producer where it left
off. Imagine a scenario whereby the producer is sending a message for each line in a file and
the application uses the line number as the publishing ID. If the application restarts
because of some necessary maintenance or even a crash, the producer can restart
from the beginning of the file: there would no duplicate messages because the producer
has a name and the application sets publishing IDs appropriately. Nevertheless,
this is far from ideal, it would be much better to restart just after the last line
the broker successfully confirmed. Fortunately this is possible thanks to
the `Producer#getLastPublishing()` method, which returns the last publishing ID for a given
producer. As the publishing ID in this case is the line number, the application can
easily scroll to the next line and restart publishing from there.

The next snippet illustrates the use of `Producer#getLastPublishing()`:

.Setting a producer where it left off
[source,java,indent=0]
--------
include::{test-examples}/ProducerUsage.java[tag=producer-queries-last-publishing-id]
--------
<1> Set a name for the producer
<2> Disable confirm timeout check
<3> Query last publishing ID for this producer and increment it
<4> Scroll to the content for the next publishing ID
<5> Set the message publishing

==== Consumer

`Consumer` is the API to consume messages from a stream.

===== Creating a Consumer

A `Consumer` instance is created with `Environment#consumerBuilder()`. The main
settings are the stream to consume from, the place in the stream to start
consuming from (the _offset_), and a callback when a message is received
(the `MessageHandler`). The next snippet shows how to create a `Consumer`:

.Creating a consumer
[source,java,indent=0]
--------
include::{test-examples}/ConsumerUsage.java[tag=producer-creation]
--------
<1> Use `Environment#consumerBuilder()` to define the consumer
<2> Specify the stream to consume from
<3> Specify where to start consuming from
<4> Define behavior on message consumption
<5> Build the consumer
<6> Close consumer after usage

The broker start sending messages as soon as the `Consumer` instance is created.

[[specifying-an-offset]]
===== Specifying an Offset

The offset is the place in the stream where the consumer starts consuming from.
The possible values for the offset parameter are the following:

* `OffsetSpecification.first()`: starting from the first available offset. If
the stream has not been <<limiting-the-size-of-a-stream,truncated>>, this
means the beginning of the stream (offset 0).
* `OffsetSpecification.last()`: starting from the end of the stream and returning
the last <<chunk-definition,chunk>> of messages immediately (if the stream is not empty).
* `OffsetSpecification.next()`: starting from the next offset to be written. Contrary
to `OffsetSpecification.last()`, consuming with `OffsetSpecification.next()`
will not return anything if no-one is publishing to the stream. The broker will start
sending messages to the consumer when messages are published to the stream.
* `OffsetSpecification.offset(offset)`: starting from the specified offset. 0 means consuming
from the beginning of the stream (first messages). The client
can also specify any number, for example the offset where it left off
in a previous incarnation of the application.
* `OffsetSpecification.timestamp(timestamp)`: starting from the messages stored
after the specified timestamp.

[[chunk-definition]]
[NOTE]
.What is a chunk of messages?
====
A chunk is simply a batch of messages. This is the storage and transportation
unit used in RabbitMQ Stream, that is messages are stored contiguously in a chunk
and they are delivered as part of a chunk. A chunk can be made of one to several
thousands of messages, depending on the ingress.
====

[[consumer-offset-tracking]]
===== Tracking the Offset for a Consumer

A consumer can track the offset it has reached in a stream. This allows a new incarnation
of the consumer to restart consuming where it left off. Offset tracking works in 2 steps:

* the consumer must have a *name*. The name is set with `ConsumerBuilder#name(String)`. The name
can be any value (under 256 characters) and is expected to be unique (from the application
point of view). Note neither the client library, nor the broker
enforces uniqueness of the name: if 2 `Consumer` Java instances share the same name, their
offset tracking will likely be interleaved, which applications usually do not expect.
* the consumer must periodically *commit the offset* it has reached so far. The way
offsets are committed depends on the commit strategy: automatic or manual.

Whatever commit strategy you use, *a consumer must have a name to be able to commit offsets*.

====== Automatic Offset Commit

The following snippet shows how to enable automatic commit with the defaults:

.Using automatic commit strategy with the defaults
[source,java,indent=0]
--------
include::{test-examples}/ConsumerUsage.java[tag=auto-commit-defaults]
--------
<1> Set the consumer name (mandatory for offset tracking)
<2> Use automatic commit strategy with defaults

The automatic commit strategy has the following available settings:

* *message count before commit*: the client will commit the offset
after the specified number of messages, right after the execution
of the message handler. _The default is every 10,000 messages_.
* *flush interval*: the client will make sure to commit the last received offset
at the specified interval. This avoids having pending, not committed offsets in
case of inactivity. _The default is 5 seconds_.

Those settings are configurable, as shown in the following snippet:

.Configuring the automatic commit strategy
[source,java,indent=0]
--------
include::{test-examples}/ConsumerUsage.java[tag=auto-commit-with-settings]
--------
<1> Set the consumer name (mandatory for offset tracking)
<2> Use automatic commit strategy
<3> Commit every 50,000 messages
<4> Make sure to commit offset at least every 10 seconds

Note the automatic commit is the default commit strategy, so if you are fine
with its defaults, it is enabled as soon as you specify a name
for the consumer:

.Setting only the consumer name to enable automatic commit
[source,java,indent=0]
--------
include::{test-examples}/ConsumerUsage.java[tag=auto-commit-only-with-name]
--------
<1> Set only the consumer name to enable automatic commit with defaults

Automatic commit is simple and provides good guarantees. It is nevertheless
possible to have more fine-grained control over offset commit by using
manual commit.

====== Manual Offset Commit

The manual commit strategy lets the developer in charge of committing offsets
whenever they want, not only after a given number of messages has been received
and supposedly processed, like automatic commit does.

The following snippet shows how to enable manual commit and how to commit
the offset at some point:

.Using manual commit with defaults
[source,java,indent=0]
--------
include::{test-examples}/ConsumerUsage.java[tag=manual-commit-defaults]
--------
<1> Set the consumer name (mandatory for offset tracking)
<2> Use manual commit with defaults
<3> Commit at the current offset on some condition

Manual commit has only one setting: the *check interval*. The client checks
that the last requested committed offset has been actually committed at the
specified interval. _The default check interval is 5 seconds_.

The following snippet shows the configuration of manual commit:

.Configuring manual commit strategy
[source,java,indent=0]
--------
include::{test-examples}/ConsumerUsage.java[tag=manual-commit-with-settings]
--------
<1> Set the consumer name (mandatory for offset tracking)
<2> Use manual commit with defaults
<3> Check last requested offset every 10 seconds
<4> Commit at the current offset on some condition

The snippet above uses `MessageHandler.Context#commit()` to commit at the
offset of the current message, but it is possible to commit anywhere
in the stream with `MessageHandler.Context#consumer()#commit(long)` or
simply `Consumer#commit(long)`.

====== Considerations On Offset Tracking

_When to commit offsets?_ Avoid committing offsets too often or, worse, for each message.
Even though offset tracking is a small and fast operation, it will
make the stream grow unnecessarily, as the broker persists offset
tracking entries in the stream itself.

A good rule of thumb is to commit every few thousands
of messages. Of course, when the consumer will restart consuming in a new incarnation, the
last tracked offset may be a little behind the very last message the previous incarnation
actually processed, so the consumer may see some messages that have been already processed.

A solution to this problem is to make sure processing is idempotent or filter out the
last duplicated messages.

'''

_Is the offset a reliable absolute value?_ Message offsets may not be contiguous.
This implies that the message at offset 500 in a stream may
not be the 501 message in the stream (offsets start at 0).
There can be different types of entries in a stream storage, a message is
just one of them. For example, committing an offset creates an offset tracking
entry, which has its own offset.

This means one must be careful when basing some decision on offset values, like
a modulo to perform an operation every X messages. As the message offsets have
no guarantee to be contiguous, the operation may not happen exactly every X messages.